{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31732237-55fe-4b75-8680-1c2f2215fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from nltk import pos_tag\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b5c07-b13e-4af6-a221-2b9661a2e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "data_dir = 'data'\n",
    "max_len = 128\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "sp_token = '[MASK]'\n",
    "\n",
    "\n",
    "with open(os.path.join(data_dir, 'target_keywords.json'), 'r') as file:\n",
    "    target_keywords = json.load(file)\n",
    "with open(os.path.join(data_dir, 'other_sentences.json'), 'r') as file:\n",
    "    other_sentences = json.load(file)\n",
    "with open(os.path.join(data_dir, 'target_sentences.json'), 'r') as file:\n",
    "    target_sentences = json.load(file)\n",
    "\n",
    "\n",
    "ns = [5] ## R_stms\n",
    "print(f\"R_STMS is {ns[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c8dc6-93c0-4a51-b05e-f78a3ca9aefd",
   "metadata": {},
   "source": [
    "### Making STMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9483755-1ba8-424a-90c3-7404d28ac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_target_sen, replaced_words, masked_target_hard_neg, replaced_words_hard_neg  = [], [], [], []\n",
    "for sen in tqdm(target_sentences):\n",
    "    words = nltk.WordPunctTokenizer().tokenize(sen)\n",
    "    noun_inds = []\n",
    "    target_inds = []\n",
    "    for ind, data in enumerate(nltk.pos_tag(words)):\n",
    "        if (data[1].startswith('N')) and (data[0].lower() not in target_keywords and data[0].isalnum()): noun_inds.append(ind) \n",
    "        elif data[0].lower() in target_keywords: target_inds.append(ind)\n",
    "\n",
    "    for target_ind in target_inds:\n",
    "        words_ = words.copy()\n",
    "        replaced_words.append(words_[target_ind]) ## sotre replaced words \n",
    "        words_[target_ind] = sp_token ##Substitute target keyword with SP token\n",
    "        masked_target_sen.append(\" \".join(words_))\n",
    "\n",
    "    for n in ns:\n",
    "        if len(noun_inds) > n-1:\n",
    "            change_inds = random.sample(noun_inds, n)\n",
    "        else:\n",
    "            change_inds = noun_inds\n",
    "            \n",
    "        for change_ind in change_inds:\n",
    "            words_ = words.copy()\n",
    "            replaced_words_hard_neg.append(words[change_ind])\n",
    "            words_[change_ind] = sp_token ##Substitute target keyword with SP token\n",
    "            masked_target_hard_neg.append(\" \".join(words_))\n",
    "len(masked_target_sen), len(replaced_words), len(masked_target_hard_neg), len(replaced_words_hard_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfc15f-33f8-481a-a044-245c45641120",
   "metadata": {},
   "source": [
    "### Making non STMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6126beb-9374-4fe4-99c4-587b28456877",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentences_other, replaced_words_other = [], []\n",
    "n=1\n",
    "for sen in tqdm(other_sentences):\n",
    "    words = nltk.WordPunctTokenizer().tokenize(sen)\n",
    "    noun_inds = []\n",
    "    for ind, data in enumerate(nltk.pos_tag(words)):\n",
    "        if (data[1].startswith('N')) and (data[0].isalnum()): noun_inds.append(ind) \n",
    "    if len(noun_inds) > n-1:\n",
    "        change_inds = random.sample(noun_inds, n)\n",
    "    else:\n",
    "        change_inds = noun_inds\n",
    "    for change_ind in change_inds:\n",
    "        words_ = words.copy()\n",
    "        replaced_words_other.append(words[change_ind])\n",
    "        words_[change_ind] = sp_token ##Substitute target keyword with SP token\n",
    "        masked_sentences_other.append(\" \".join(words_))\n",
    "len(masked_sentences_other), len(replaced_words_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33febb15-a284-41f0-98f0-97a470bc771f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cd5bb-6521-4d46-a58a-8a8ccf30181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length= max_len)\n",
    "\n",
    "def get_target_word_embeddings(sentences_with_mask, target_words):\n",
    "    # Replace [MASK] with target words\n",
    "    sentences = [sentence.replace(\"[MASK]\", target_word) for sentence, target_word in zip(sentences_with_mask, target_words)]\n",
    "\n",
    "    # Tokenize sentences with [MASK]\n",
    "    inputs_with_mask = tokenizer(sentences_with_mask, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Identify positions of [MASK] tokens\n",
    "    mask_positions = [torch.nonzero(input_ids == tokenizer.mask_token_id, as_tuple=False)[0].item() for input_ids in inputs_with_mask['input_ids']]\n",
    "    mask_positions = torch.tensor(mask_positions, dtype=torch.long, device=device)\n",
    "\n",
    "    # Run model on sentences with target words\n",
    "    with torch.no_grad():\n",
    "        tokenized_inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        outputs = model(**tokenized_inputs)\n",
    "        embeddings = outputs.last_hidden_state.detach()\n",
    "\n",
    "    # Extract embeddings at the [MASK] positions\n",
    "    target_embeddings = embeddings[torch.arange(embeddings.size(0)), mask_positions].cpu()\n",
    "\n",
    "    # Explicitly delete large objects\n",
    "    del outputs\n",
    "    del embeddings\n",
    "    del tokenized_inputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return target_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec83bf-0745-4899-ae5b-54bfab22ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(os.path.join(data_dir,'BERT_pretrained_reddit'))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c03fa8-3135-4bcf-98a3-c73378ec712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [2] ## R_nonstms\n",
    "print(f\"R_nonSTMS is {ratios[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3abc21-0712-4c58-87e2-eb4ec0ecacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ratio in ratios:\n",
    "    masked_sentences_other_sampled_words = random.sample(list(zip(masked_sentences_other,replaced_words_other)), ratio*len(masked_target_sen))\n",
    "    masked_sentences_other_sampled = []\n",
    "    replaced_word_other_sampled = []\n",
    "    for d in masked_sentences_other_sampled_words:\n",
    "        masked_sentences_other_sampled.append(d[0])\n",
    "        replaced_word_other_sampled.append(d[1])\n",
    "    for maxn in ns:\n",
    "        print(ratio, maxn)\n",
    "        print(len(masked_target_sen), len(masked_target_hard_neg), len(masked_sentences_other_sampled))\n",
    "\n",
    "        labels = [1]*len(masked_target_sen)+[0]*len(masked_target_hard_neg)+[0]*len(masked_sentences_other_sampled)\n",
    "        words = replaced_words+replaced_words_hard_neg+replaced_word_other_sampled\n",
    "\n",
    "        train_text, valid_text, train_label, valid_label, train_words, valid_words = train_test_split(masked_target_sen+masked_target_hard_neg+masked_sentences_other_sampled, labels, words, train_size=0.8, shuffle=True)\n",
    "\n",
    "        train_embs, valid_embs = [], []\n",
    "\n",
    "        print('Extracting embs for Train')\n",
    "        for data in tqdm(np.array_split(list(zip(train_text, train_words)), 1000)): ##Splitting for GPU memory\n",
    "            train_embs.extend(get_target_word_embeddings(list(data[:,0]), list(data[:,1])))\n",
    "\n",
    "        print('Extracting embs for Valid')\n",
    "        for data in tqdm(np.array_split(list(zip(valid_text, valid_words)), 1000)): ##Splitting for GPU memory\n",
    "            valid_embs.extend(get_target_word_embeddings(list(data[:,0]), list(data[:,1])))\n",
    "\n",
    "\n",
    "        train = datasets.Dataset.from_dict({'text': train_text, 'label': train_label, 'word': train_words, 'emb': train_embs}) ## datasets automatically tensor or numpy to list, so later, change its type to tensor\n",
    "        valid = datasets.Dataset.from_dict({'text': valid_text, 'label': valid_label, 'word': valid_words, 'emb': valid_embs})\n",
    "        dataset = datasets.DatasetDict({'train':train, 'valid':valid})    \n",
    "\n",
    "\n",
    "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        print(f'Emb_Dataset_ratio{ratio}_max{maxn}_{random_seed}')\n",
    "        \n",
    "        tokenized_dataset.save_to_disk(os.path.join(data_dir, f'Emb_Dataset_ratio{ratio}_max{maxn}_{random_seed}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cd287-5826-4398-aed5-df8eb4e45de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedis",
   "language": "python",
   "name": "jedis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
